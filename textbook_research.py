# -*- coding: utf-8 -*-
"""textbook research.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fer5BH8-y0gAeM_pGNpixHiMjPKD8dWt
"""

import re
from google.colab import files

def clean_document(text):
    """
    清理文档中的无关信息
    """
    # 1. 移除页码标号 (如 "第7页", "p78" 等)
    text = re.sub(r'第\d+页', '', text)
    text = re.sub(r'p\d+', '', text)

    # 2. 移除括号中的数字引用 (如 "(第一册第二课)")
    text = re.sub(r'（[^）]*\d+[^）]*）', '', text)
    text = re.sub(r'\([^)]*\d+[^)]*\)', '', text)

    # 3. 定义要移除的虚词列表
    meaningless_words = ['之', '乎', '者', '也', '矣', '焉', '哉', '兮', '耶', '欤']

    # 4. 移除这些虚词，但要注意不要移除成语或固定搭配中的这些字
    # 这里使用简单的替换，更复杂的需要词典
    for word in meaningless_words:
        text = text.replace(word, '')

    # 5. 清理多余的空白字符
    text = re.sub(r'\s+', ' ', text)  # 多个空格合并为一个
    text = re.sub(r'\n\s*\n', '\n\n', text)  # 保留段落分隔

    # 6. 移除其他格式标记 (如 "【推究及应用】" 等，但保留内容)
    text = re.sub(r'【[^】]*】', '', text)
    text = re.sub(r'\[[^\]]*\]', '', text)

    # 7. 清理开头和结尾的空白
    text = text.strip()

    return text

def main():
    # 读取文件内容
    file_path = "（textbook）Changes in the education of overseas Chinese in southeast Asia after the 918 incident.txt"

    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()

        print("原始文档长度:", len(content))
        print("\n原始文档前500字符:")
        print(content[:500])

        # 清理文档
        cleaned_content = clean_document(content)

        print("\n清理后文档长度:", len(cleaned_content))
        print("\n清理后文档前500字符:")
        print(cleaned_content[:500])

        # 保存清理后的文档
        output_file = "cleaned_document.txt"
        with open(output_file, 'w', encoding='utf-8') as file:
            file.write(cleaned_content)

        print(f"\n清理完成！已保存为: {output_file}")

        # 下载文件到本地
        files.download(output_file)

    except FileNotFoundError:
        print(f"文件 {file_path} 未找到，请确保文件已上传到Colab环境")
        # 如果文件不存在，提供上传选项
        uploaded = files.upload()
        if uploaded:
            file_name = list(uploaded.keys())[0]
            with open(file_name, 'r', encoding='utf-8') as file:
                content = file.read()

            cleaned_content = clean_document(content)

            output_file = "cleaned_document.txt"
            with open(output_file, 'w', encoding='utf-8') as file:
                file.write(cleaned_content)

            print(f"清理完成！已保存为: {output_file}")
            files.download(output_file)

# 更精确的清理函数，保留重要内容
def advanced_clean_document(text):
    """
    更精确的文档清理，保留重要内容
    """
    # 分割成行处理
    lines = text.split('\n')
    cleaned_lines = []

    for line in lines:
        # 跳过纯页码行
        if re.match(r'^第\d+页$', line.strip()):
            continue

        # 跳过纯数字行
        if re.match(r'^\d+$', line.strip()):
            continue

        # 移除行内的页码引用
        line = re.sub(r'第\d+页', '', line)
        line = re.sub(r'p\d+', '', line)

        # 移除括号中的课程引用但保留内容
        line = re.sub(r'（[^）]*第一?册第?\d+课[^）]*）', '', line)
        line = re.sub(r'\([^)]*第一?册第?\d+课[^)]*\)', '', line)

        # 移除格式标记但保留其后的内容
        line = re.sub(r'【[^】]*】', '', line)
        line = re.sub(r'\[[^\]]*\]', '', line)

        # 移除虚词（更保守的方式）
        # 只在单独出现或特定位置移除
        meaningless_words = ['之', '乎', '者', '也', '矣', '焉', '哉', '兮', '耶', '欤']
        for word in meaningless_words:
            # 只在单词边界处替换
            line = re.sub(r'\b' + word + r'\b', '', line)

        # 清理空白
        line = re.sub(r'\s+', ' ', line).strip()

        if line:  # 只保留非空行
            cleaned_lines.append(line)

    return '\n'.join(cleaned_lines)

# 可以选择使用更精确的清理方法
def main_advanced():
    file_path = "（textbook）Changes in the education of overseas Chinese in southeast Asia after the 918 incident.txt"

    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()

        print("使用精确清理方法...")
        cleaned_content = advanced_clean_document(content)

        print("清理后文档长度:", len(cleaned_content))
        print("\n清理后文档预览:")
        print(cleaned_content[:1000])

        # 保存清理后的文档
        output_file = "cleaned_document_advanced.txt"
        with open(output_file, 'w', encoding='utf-8') as file:
            file.write(cleaned_content)

        print(f"\n精确清理完成！已保存为: {output_file}")
        files.download(output_file)

    except FileNotFoundError:
        print("文件未找到，请先上传文件")
        uploaded = files.upload()
        if uploaded:
            file_name = list(uploaded.keys())[0]
            with open(file_name, 'r', encoding='utf-8') as file:
                content = file.read()

            cleaned_content = advanced_clean_document(content)

            output_file = "cleaned_document_advanced.txt"
            with open(output_file, 'w', encoding='utf-8') as file:
                file.write(cleaned_content)

            print(f"精确清理完成！已保存为: {output_file}")
            files.download(output_file)

# 运行程序
if __name__ == "__main__":
    print("选择清理方法:")
    print("1. 基础清理")
    print("2. 精确清理（推荐）")

    choice = input("请输入选择 (1 或 2): ").strip()

    if choice == "1":
        main()
    else:
        main_advanced()

with open("cleaned_document_advanced.txt", "r") as f:
  text = f.read()
print(f"Original length: {len(text)}")

!pip install qhchina
import qhchina
import os
import re
import jieba
from tqdm.auto import tqdm

with open("cleaned_document_advanced.txt", "r") as f:
  text = f.read()

is_premodern_chinese = False
chinese_only = re.compile(r'^[\u4e00-\u9fff]+$')

all_sentences = []

lines = [line for line in text.split("\n") if len(line) > 5]
for line in tqdm(lines):
  sentences = re.split(r"[。！？]", line)
  if is_premodern_chinese:
    sentences_split_into_words = [[char for char in sent if chinese_only.match(char)] for sent in sentences if len(sent) > 10]
  else:
    sentences_split_into_words = [[word for word in jieba.lcut(sent) if chinese_only.match(word)] for sent in sentences if len(sent) > 10]
  all_sentences.extend(sentences_split_into_words)

print(f"Total sentences: {len(all_sentences)}")

from qhchina.analytics.collocations import find_collocates
from qhchina.helpers import load_stopwords

if is_premodern_chinese:
  stopwords = load_stopwords("zh_cl_tr")
else:
  stopwords = load_stopwords("zh_sim")

filters = {'max_p': 0.05, 'stopwords': stopwords, "min_word_length": 2}

collocations = find_collocates(all_sentences,
                               target_words=["中国"],
                               method='window',
                               horizon=5,
                               filters=filters,
                               as_dataframe=True,
                               alternative='greater')

collocations

from IPython.display import HTML

html_content = """
<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>中国相关性分析图表</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        canvas {
            max-width: 600px;
            margin: auto;
        }
    </style>
</head>
<body>
    <h2>与“中国”相关的词汇及其p-value</h2>
    <canvas id="myChart"></canvas>
    <script>
        const ctx = document.getElementById('myChart').getContext('2d');
        const myChart = new Chart(ctx, {
            type: 'bar',
            data: {
                labels: [
                    '日本', '人到', '外国人', '地方',
                    '路程', '可爱', '外人', '工商业',
                    '现在', '各国', '问题', '留学',
                    '以后', '马哥', '博罗', '欧洲人',
                    '敕封', '关系'
                ],
                datasets: [{
                    label: 'p-value',
                    data: [
                        0.000201, 0.000675, 0.000891, 0.001663,
                        0.002523, 0.007710, 0.007710, 0.007710,
                        0.009341, 0.009906, 0.011026, 0.011026,
                        0.022179, 0.041035, 0.041035, 0.041035,
                        0.045774
                    ],
                    backgroundColor: 'rgba(75, 192, 192, 0.2)',
                    borderColor: 'rgba(75, 192, 192, 1)',
                    borderWidth: 1
                }]
            },
            options: {
                scales: {
                    y: {
                        beginAtZero: true,
                        title: {
                            display: true,
                            text: 'p-value'
                        }
                    }
                }
            }
        });
    </script>
</body>
</html>
"""

display(HTML(html_content))



from qhchina.analytics.collocations import find_collocates
from qhchina.helpers import load_stopwords

if is_premodern_chinese:
  stopwords = load_stopwords("zh_cl_tr")
else:
  stopwords = load_stopwords("zh_sim")

filters = {'max_p': 0.05, 'stopwords': stopwords, "min_word_length": 2}

collocations = find_collocates(all_sentences,
                               target_words=["日本"],
                               method='window',
                               horizon=5,
                               filters=filters,
                               as_dataframe=True,
                               alternative='greater')

collocations

from IPython.display import HTML

html_content = """
<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>与“日本”相关的词汇及其p-value</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        canvas {
            max-width: 600px;
            margin: auto;
        }
    </style>
</head>
<body>
    <h2>与“日本”相关的词汇及其p-value</h2>
    <canvas id="myChart"></canvas>
    <script>
        const ctx = document.getElementById('myChart').getContext('2d');
        const myChart = new Chart(ctx, {
            type: 'bar',
            data: {
                labels: [
                    '传播', '朝鲜', '中国', '安南',
                    '人到', '唐时', '敕封', '俄国',
                    '近年来', '辽东半岛', '输入',
                    '第一七', '维新', '留学',
                    '三课', '条件', '文化', '向来',
                    '明朝'
                ],
                datasets: [{
                    label: 'p-value',
                    data: [
                        0.0000156, 0.0000213, 0.0000556, 0.0002494,
                        0.0002494, 0.0002494, 0.0009506, 0.0022653,
                        0.0039727, 0.0039727, 0.0039727, 0.0039727,
                        0.0039727, 0.0043193, 0.0109972, 0.0114194,
                        0.0192539, 0.0218882, 0.0349702
                    ],
                    backgroundColor: 'rgba(75, 192, 192, 0.2)',
                    borderColor: 'rgba(75, 192, 192, 1)',
                    borderWidth: 1
                }]
            },
            options: {
                scales: {
                    y: {
                        beginAtZero: true,
                        title: {
                            display: true,
                            text: 'p-value'
                        }
                    }
                }
            }
        });
    </script>
</body>
</html>
"""

display(HTML(html_content))